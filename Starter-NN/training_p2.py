# CLEAR GOAL: Minimize the loss of the neural network
    # We know that we can change the NN's weights and biases to influence its preds, but how can we do that in a way that minimizes loss?
    
# multivariable calculus moment...

# We can wrote loss as a multivar function --> L(w1, w2, w3, w4, w5, w6, b1, b2, b3)

# Imagine we want to modify w1. How would L() change if we changed w1?
    # Use partial derivatives

# After some math... We can break a partial derivative down into serveral easier-to-calculate parts
# This system of calculating partial derivatives by working backwards is called BACKPROPOGATION



